{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#기초 웹 크롤링 예시 - 이메일 크롤링\n",
    "\n",
    "#이메일 같은 로그인이 필요한 웹사이트들을 크롤링 하는데에는 뷰티풀수프로 할 수 없다.\n",
    "#뷰티풀수프에는 로그인 기능이 없기 때문이다.\n",
    "#크롬 드라이버와 크롬 브라우저를 사용해 이메일 크롤링을 진행한다.\n",
    "\n",
    "from selenium import webdriver\n",
    "import time\n",
    "\n",
    "browser = webdriver.Chrome(\".\\chromedriver\")\n",
    "#크롬 함수는 크롬 브라우저를 실행시킨다. 셀레니움 모듈의 기능은 브라우저 창을 파이썬 명령어로 제어하는 것이다.\n",
    "#즉 사용자가 키보드를 입력하는 것처럼 해당하는 행동들을 명령어로 수행가능한 것이다.\n",
    "\n",
    "browser.get(\"https://logins.daum.net/accounts/signinform.do?url=https%3A%2F%2Fwww.daum.net%2F\")\n",
    "#인터넷 이메일 접속시 다음을 예시로 들었다. 위의 url은 다음 페이지의 로그인하는 페이지 주소 이다.\n",
    "#위처럼 브라우저 겟을 넣어놓으면 그 브라우저로 이동하게 될 것이다.\n",
    "\n",
    "id = browser.find_element_by_css_selector(\"input#id\")\n",
    "#CSS 선택자로 요소를 찾아주는 함수이다.\n",
    "\n",
    "id.send_keys(\"아이디\")\n",
    "#send_keys는 해당되는 요소에 입력 하는 함수이다.\n",
    "\n",
    "pw = browser.find_element_by_css_selector(\"input#inputPwd\")\n",
    "pw.send_keys(\"비밀번호\")\n",
    "\n",
    "#이후 로그인 버튼을 클릭해야 할 것이다. 이를 위해 로그인 버튼을 찾아서 클릭하게 만들자.\n",
    "\n",
    "browser.find_element_by_css_selector(\"button#loginBtn\").click()\n",
    "#뒤에 click() 함수를 더해서 해당되는 요소를 클릭하도록 만들 수 있다.\n",
    "time.sleep(3)\n",
    "\n",
    "#이후 이메일 함으로 이동한다.\n",
    "browser.get(\"https://mail.daum.net/\")\n",
    "time.sleep(2)\n",
    "\n",
    "#셀레니움은 뷰티풀수프와 다르게 html코드를 따로 받아와서 정리 후 컴퓨터에게 알려주지 않아도\n",
    "#간편하게 진행할 수 있다.\n",
    "\n",
    "title = browser.find_elements_by_css_selector(\"strong.tit_subject\")\n",
    "for i in title:\n",
    "    print(i.text)\n",
    "    \n",
    "#뷰티풀 수프에서는 .string으로 표현했다면 셀레니움은 .text로 표현한다.\n",
    "#이후 browser.close()로 크롬 브라우저 창을 닫아준다.\n",
    "\n",
    "browser.close()\n",
    "\n",
    "#셀레니움은 파이썬에 비해 비교적 무거운 크롬을 실행하게 때문에 크롬과 파이썬 코드 사이에서 속도 차이가 발생한다.\n",
    "#즉 , 크롬 브라우저에서는 아직 작업이 진행중인데 파이썬 프로그램은 끝나버린 것이다.\n",
    "#그래서 time.sleep() 으로 적당히 딜레이를 주어서 속도를 비슷하게 맞출 수 있다.\n",
    "#time.sleep() 은 쉽게 생각하면 웹페이지 이동이 생길 때 마다 넣어준다고 생각 할 수도 있고 , 2 ~ 3 초 정도면 적당하다.\n",
    "\n",
    "#셀레니움과 뷰티풀 수프의 비교점\n",
    "#1. 셀레니움은 거의 모든곳을 크롤링 할 수 있다.\n",
    "#2. 셀레니움은 뷰티풀 수프에 비해 코드가 간결해 코드짜기가 편하다.\n",
    "#3. 셀레니움은 뷰티풀 수프에 비해 상당히 느리다.\n",
    "#4. 셀레니움은 time.sleep()을 때에 따라 사용해야 하고 , 인터넷 연결 상태등에 따라 조정해야 할 수도 있다.\n",
    "#--> 셀레니움을 사용하는 경우 / 로그인 , 글자 입력 , 버튼 클릭등을 사용할 때 / 동적인 웹페이지로 부터 크롤링을 진행할 떄"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#기초 웹 크롤링 예시 - 셀레니움 로그인 기능 활용하기\n",
    "#import pyperclip\n",
    "\n",
    "#위쪽처럼 로그인 페이지에서 로그인도 가능하고 셀레니움에서 제공하는 기능을 통해서 로그인도 가능하다.\n",
    "\n",
    "option = webdriver.ChromeOptions()\n",
    "#웹드라이버의 크롬옵션스를 통해서 옵션들을 사용할 수 있다.\n",
    "\n",
    "option.add_argument(\"headless\")\n",
    "#headless라는 옵션을 더하게 된다. 이후 아래와 같게 브라우저를 실행할 때 옵션을 매개변수로 받아 실행할 수 있다.\n",
    "\n",
    "browser = webdriver.Chrome(\".\\chromedriver\", options=option)\n",
    "#headless라는 옵션은 크롬브라우저가 실행되고 있는데 브라우저 창이 물리적으로 보이지 않는 것이다.\n",
    "#이런 식으로 구현하면 브라우저 창이 물리적으로 실행되지 않아 속도가 조금이라도 빨라질 수 있다.\n",
    "\n",
    "#네이버의 경우 다음과 다르게 셀레니움을 인식해서 식별문자를 입력하도록 했다.\n",
    "#네이버는 셀레니움을 사람과 다르게 너무 빠르게 아이디를 입력하고 비밀번호를 입력하는 것으로 일반 사용자와\n",
    "#셀레니움을 구분한다. 그래서 네이버 같은 셀레니움이 사용되지 않는 경우에 사용하는 아이디어가 있다.\n",
    "#아이디를 복사 붙여넣기로 입력하는 아이디어 이다.\n",
    "#인풋을 받으면 pyperclip과 셀레니움 키 입력으로 컨트롤 + v 를 해서 붙여넣기를 하는 것이다.\n",
    "#자세한 사용법은 pyperclip과 셀레니움 키입력을 통해서 확인해보자.\n",
    "#네이버 카페나 블로그 등에서 크롤링을 하려면 로그인이 필요하기 떄문에 이를 이용해서 문제를 해결할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "와.. 내일 출근해야하는데 이걸봐도 잠이안온다 큰일이네\n",
      "시ㅋ발 저걸 조상님덜이 원시인시절\n",
      "초원에서 저런거 만났다고 생각하니까\n",
      "신을 안믿을 수가 없다 ㅋㅋㅋㅋㅋ\n",
      "우리나라는 지진, 토네이도 위험도 거의 없고, 기후도 정말 좋은데 바로 옆에 중국이 있어서 많은 장점을 다 상쇄시키지.\n",
      "실제로보면 존나무서울듯 ㅋㅋㅋㅋ 하늘이 저렇게 높은데 땅에서부터 하늘까지 제2롯데월드보다 높은 크기로 회오리가 있다 생각하면 ㅋㅋㅋㅋ\n",
      "목성에선 저런 토네이도가 지구 5개 크기로 수십년째 지속되고 있다는 게 참 신기 ㅋㅋㅋ\n",
      "'못참지오그래픽'\n",
      "3:28 미친놈인가?;;;;\n",
      "자연의 힘은 위대하고 ㅈ나무섭다\n",
      "토네이도에 중저음에 비장함이 느껴지는 나레이션.. 이건 못참는다\n",
      "0:40 앞에 토네이토 있는데 그냥 악셀밟는거 실화야?\n",
      "뒤지고싶은건지 ㅋㅋ\n",
      "와...진짜 우린 산이 많아서 토네이도가 없으니까... 한국에서 볼일은 아예없겠지만 영상으로만 접해도 진짜 무섭다 ;;;\n",
      "내셔날지오그래픽: 도로씨의 대중교통은 무엇일까\n",
      "1:47 목소리 개 지린다 ㅋㅋㅋㅋ개좋음\n",
      "아무 일도 없이 출근하다가 도로에서 시커먼 기둥이 빠르게 다가오는 걸 무력하게 바라보고만 있을 수밖에 없는 상황이란 어떤 느낌일까...\n",
      "상상도 할 수 없음...\n",
      "토네이도가 너무 커서 하늘로 보이는게 소름...\n",
      "미국 중남부쪽은 땅은 기름지고 좋은데 토네이도때문에..ㅈㄴ무섭겠다...\n",
      "울나라로 오는 태풍은 귀요미 수준이다.ㅋㅋㅋ\n",
      "'가장 강력한'\n",
      "'초대형'\n",
      "'토네이도'\n",
      "아 큰일이네\n",
      "내일 새벽출근인데 넷지오가 왜 떠\n",
      "토네이도라니.. 우주만큼 궁금하잖아..\n",
      "내셔널 티비채널에도 자막 넣어주시면 안될까요?\n",
      "3:50 멋있네요\n"
     ]
    }
   ],
   "source": [
    "#기초 웹 크롤링 예시 - 유튜브 댓글 수집\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "#스크롤을 내리기 위한 모듈 , 위쪽에서는 키 입력을 위해서 사용했다.\n",
    "\n",
    "#유튜브는 동적 웹사이트로 마우스 스크롤을 내릴때 마다 새로운 댓글이 나타나게 된다.\n",
    "#이러한 특정 기능에 의해 내용이 추가 될 수 있는 동적 웹사이트들은 뷰티풀 수프를 사용할 수 없다.\n",
    "#따라서 이 경우 셀레니움을 사용하여 크롤링을 진행한다.\n",
    "\n",
    "browser = webdriver.Chrome(\".\\chromedriver\")\n",
    "browser.get(\"https://www.youtube.com/watch?v=bQ9jPJurxmM\")\n",
    "time.sleep(4)\n",
    "\n",
    "#유튜브 댓글이 보이는 경우가 나누어지는데 노트북의 해상도에 따라서 스크롤을 다 내려야 보이기도 하고\n",
    "#내리지 않아도 보이기도 한다. 이처럼 다양한 웹 페이지를 크롤링 할때는 본인 상황에 맞게 구현해야 한다.\n",
    "\n",
    "browser.find_element_by_css_selector(\"html\").send_keys(Keys.PAGE_DOWN)\n",
    "time.sleep(3)\n",
    "#send_keys는 사용할 때 언제나 html요소를 하나 가져와서 사용해야 한다.\n",
    "#아무 html 요소를 써도 되지만 너무 아무거나 사용하는 것은 좋지 않다.\n",
    "#그래서 모든 html 페이지가 가지고 있는 html 태그를 사용한다.\n",
    "\n",
    "#browser.find_element_by_css_selector(\"html\").send_keys(Keys.END) --> 스크롤을 끝까지 내릴때는 END를 사용한다.\n",
    "\n",
    "comments = browser.find_elements_by_css_selector(\"#content-text\")\n",
    "#댓글을 수집할거니까 댓글의 태그명을 확인하고 수집하자. 태그명을 생략하고 위처럼 id 부분만 써도 된다.\n",
    "\n",
    "#원하는 수만큼 댓글을 불러오기 위해서 한번의 스크롤에 몇개의 댓글이 불러와지는지 확인해보면\n",
    "#한번에 20개씩 생성되는 것을 볼 수 있다.\n",
    "#카운터 변수를 통해 무한 while 루프를 돌려서 원하는 개수가 되면 빠져나오도록 코딩 할 수 있다.\n",
    "#즉 카운터 변수가 20개를 넘으면 스크롤 내리기 함수를 실행해 내리고 다시 받아오도록 할 수 있다.\n",
    "\n",
    "#while True / cnt = 0:\n",
    "#print(코멘트) cnt += 1\n",
    "#if cnt % 20:\n",
    "#스크롤 내리고 / time.sleep(3) 쉬고 / comments = find.elements로 받아와서 다시 코멘트 리스트 채우고\n",
    "\n",
    "for i in comments:\n",
    "    print(i.text)\n",
    "#위처럼 for문으로 하면 한스크롤 단위인 20개를 읽어온다.\n",
    "\n",
    "#만약 while문을 사용해서 모든 댓글을 크롤링 한 경우 출력 단위가 작으면 오류가 날 수 도 있다.\n",
    "#try - except 문으로 예외처리를 해서 깔끔하게 표현해도 되고 그냥 두어도 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "나르\n",
      "세트\n",
      "사일러스\n",
      "피오라\n",
      "이렐리아\n",
      "말파이트\n",
      "아트록스\n",
      "카밀\n",
      "아칼리\n",
      "그웬\n",
      "레넥톤\n",
      "제이스\n",
      "녹턴\n",
      "잭스\n",
      "문도 박사\n",
      "오른\n",
      "오공\n",
      "쉔\n",
      "가렌\n",
      "볼리베어\n",
      "리븐\n",
      "다리우스\n",
      "티모\n",
      "나서스\n",
      "모데카이저\n",
      "리 신\n",
      "케넨\n",
      "베인\n",
      "갱플랭크\n",
      "클레드\n",
      "루시안\n",
      "트린다미어\n",
      "뽀삐\n",
      "요릭\n",
      "트런들\n",
      "라이즈\n",
      "일라오이\n",
      "퀸\n",
      "요네\n",
      "럼블\n",
      "블라디미르\n",
      "야스오\n",
      "우르곳\n",
      "마오카이\n",
      "탐 켄치\n",
      "사이온\n",
      "케일\n",
      "렝가\n",
      "워윅\n",
      "하이머딩거\n",
      "신지드\n",
      "자크\n",
      "초가스\n",
      "판테온\n",
      "세주아니\n"
     ]
    }
   ],
   "source": [
    "#기초 웹 크롤링 예시 - 롤 카운터 챔프 크롤링\n",
    "\n",
    "your_champ = input()\n",
    "browser = webdriver.Chrome(\".\\chromedriver\")\n",
    "browser.get(\"https://www.op.gg/champion/statistics\")\n",
    "time.sleep(3)\n",
    "\n",
    "champ = browser.find_elements_by_css_selector(\"div.champion-index__champion-item__name\")\n",
    "for i in champ:\n",
    "    if i.text == your_champ:\n",
    "        i.click()\n",
    "        break\n",
    "#만약 내가 찾는 챔피언과 같다면 클릭하고 break 하는 것이다.\n",
    "time.sleep(3)\n",
    "\n",
    "#가끔 웹사이트들 중에서 클릭 하거나 안하거나 등등 다양한 상태에서 태그가 바뀌는 경우가 있다.\n",
    "#해당 경우를 확인하고 원하는 태그를 찾아햐 한다.\n",
    "browser.find_element_by_css_selector(\"li.champion-stats-menu__list__item.champion-stats-menu__list__item--red.tabHeader > a\").click()\n",
    "time.sleep(1)\n",
    "\n",
    "#div.champion-matchup-list__champion > span 태그로 카운터 챔피언 이름을 받아오려 하는데 \n",
    "#div 아래의 span 요소가 여러개 이다. 이럴때는 div.champion-matchup-list__champion > span 하고 뒤에 :nth-child(2)\n",
    "#를 붙여서 하면 된다. div.champion-matchup-list__champion > span:nth-child(2) 뒤쪽에 몇번째 자손을 찾는지에 대한 coin것이다.\n",
    "\n",
    "counter = browser.find_elements_by_css_selector(\"div.champion-matchup-list__champion > span:nth-child(2)\")\n",
    "for i in counter:\n",
    "    print(i.text)\n",
    "browser.close()\n",
    "\n",
    "#추가적으로 op.gg는 어느 메뉴를 클릭하던지 url주소가 같다. 이런 경우에는 뷰티풀수프를 사용할 수 없다.\n",
    "#그런데 크롤링을 잘 하는 사람들은 셀레니움을 거의 사용하지 않는다. 셀레니움을 꼭 써야만 할 것같은 사이트들도\n",
    "#크롤링 속도를 높이기 위해 뷰티풀 수프로 어떻게든 하는 방법을 찾는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "나르\n",
      "Sett\n",
      "Sylas\n",
      "Fiora\n",
      "Irelia\n",
      "Malphite\n",
      "Aatrox\n",
      "Camille\n",
      "Akali\n",
      "Gwen\n",
      "Renekton\n",
      "Jayce\n",
      "Nocturne\n",
      "Jax\n",
      "Dr. Mundo\n",
      "Ornn\n",
      "Wukong\n",
      "Shen\n",
      "Garen\n",
      "Volibear\n",
      "Riven\n",
      "Darius\n",
      "Teemo\n",
      "Nasus\n",
      "Mordekaiser\n",
      "Lee Sin\n",
      "Kennen\n",
      "Vayne\n",
      "Gangplank\n",
      "Kled\n",
      "Lucian\n",
      "Tryndamere\n",
      "Poppy\n",
      "Yorick\n",
      "Trundle\n",
      "Ryze\n",
      "Illaoi\n",
      "Quinn\n",
      "Yone\n",
      "Rumble\n",
      "Vladimir\n",
      "Yasuo\n",
      "Urgot\n",
      "Maokai\n",
      "Tahm Kench\n",
      "Sion\n",
      "Kayle\n",
      "Rengar\n",
      "Warwick\n",
      "Heimerdinger\n",
      "Singed\n",
      "Zac\n",
      "Cho'Gath\n",
      "Pantheon\n",
      "Sejuani\n"
     ]
    }
   ],
   "source": [
    "#기초 웹 크롤링 예시 - 롤 카운터 챔프 크롤링 / 뷰티풀 수프\n",
    "\n",
    "import urllib.request as req\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "yc = input()\n",
    "headers = req.Request(\"https://www.op.gg/champion/statistics\", headers={\"Accept-Language\":\"ko-KR\"})\n",
    "code = req.urlopen(headers)\n",
    "soup = BeautifulSoup(code, \"html.parser\")\n",
    "\n",
    "#뷰티풀 수프로 구현하기 위해서는 url과 html 코드를 잘 보고 사용할 수 있도록 확인해야 한다.\n",
    "#op.gg의 경우는 잘 확인하면 각 챔피언이 공통된 요소를 가지고 있었고 그 안을 살펴보니 url이 링크 되어 있다.\n",
    "#이를 이용해서 크롤링을 진행한다.\n",
    "\n",
    "champs = soup.select(\"div.champion-index__champion-list > div\")\n",
    "for i in champs:\n",
    "    if i.attrs[\"data-champion-name\"] == yc:\n",
    "        #위의 경우 챔피언 이름을 어떻게 받아올까 확인해보면 data-champion-name에 있기 때문에\n",
    "        #이미지에서 소스 가져오듯이 챔피언 이름을 뽑아 낸 것이다.\n",
    "        a = i.select_one(\"a\")\n",
    "        #이후 a로 링크 되어있으니까 a 요소를 받아온다.\n",
    "        champ_url = \"https://www.op.gg\" + a.attrs[\"href\"]\n",
    "        break\n",
    "\n",
    "#확인 해보니 챔피언 이름이 영문자로 받아져 온다. 위쪽에서 헤더를 통해 한글로 받아올 수 있도록 요청하자.\n",
    "\n",
    "headers1 = req.Request(champ_url, headers={\"Accept-Language\":\"ko-KR\"})\n",
    "code1 = req.urlopen(headers1)\n",
    "soup1 = BeautifulSoup(code1, \"html.parser\")\n",
    "\n",
    "#카운터 주소도 확인해서 같게 링크를 a href 로 찾아서 받아와 접근하면 된다.\n",
    "\n",
    "counter_champ = soup1.select_one(\"li.champion-stats-menu__list__item.champion-stats-menu__list__item--red.tabHeader > a\")\n",
    "counter_url = \"https://www.op.gg\" + counter_champ.attrs[\"href\"]\n",
    "\n",
    "code2 = req.urlopen(counter_url)\n",
    "soup = BeautifulSoup(code2, \"html.parser\")\n",
    "counters = soup.select(\"div.champion-matchup-list__champion > span:nth-child(2)\")\n",
    "for i in counters:\n",
    "    print(i.string)\n",
    "\n",
    "#이처럼 뷰티풀 수프를 사용한 빠른 크롤링은 해당 요소를 어떻게 뽑아낼지 어떤 부모나 자손으로 구별할 수 있는지 ,\n",
    "#또 어떻게 해당 url에 접근할 수 있는지를 확인해서 모두 종합해 구성하면 된다.\n",
    "#물론 뷰티풀 수프로 불가능한 것들도 있다. 그런 경우 셀레니움을 적절히 활용할 필요가 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "南北統一(남북통일) 自主(자주)·平和(평화)원칙 합의\n",
      "祕密接觸(비밀접촉)경위\n",
      "南北(남북) 공동 성명 全文(전문)\n",
      "1972年(년)7月(월)4日(일) 朴大統領(박대통령)、國會(국회)에公翰(공한)\"金總理(김총리)보내自進報告(자진보고)\"\n",
      "너무 큰期待禁物(기대금물) 金總理(김총리)논평\n",
      "與野(여야) 論評(논평)발표\n",
      "새로운時代(시대)에 알맞게 法(법)·制度面(제도면)의 修正(수정)필요 李情報部長(이정보부장)회견\n",
      "「北韓(북한)」으로 公式呼稱(공식호칭) 共同聲明(공동성명)선地名(지명)만使用(사용)\n",
      "오늘의 紙面(지면)\n",
      "南北(남북)공동聲明(성명) ?外(외)반향自主的協商(자주적협상)찬양 豫想外(예상외)빠른解氷(해빙)\"가장 고무적현상 美國(미국)\n",
      "韓半島(한반도) 緊張緩和(긴장완화)징후\n",
      "亞州政策(아주정책)에 反映(반영) 日本(일본)\n",
      "民團(민단)·朝聯(조련) 모두 흥분\n",
      "두韓國(한국)사이 解氷(해빙) 프랑스\n",
      "多極化(다극화) ???代(대)의 놀라운 새局面南北(국면남북)공동聲明(성명)의 衝擊(충격)\n",
      "北韓(배한)서도 報道(보도)\n",
      "李部長會見一面(이부장회현일면)서계속\n",
      "?????通電話(통전화)가설에 관한 合意書(합의서)\n",
      "서울~平壤(평양)가설…指導者(지도자)끼리 緊急通話(긴급통화)로 우발戰豫防(전예방) 가능\n",
      "南北(남배)공동聲明(성명) 설명\n",
      "駐韓外交使節(주한외교사절)들에 南北(남북)공동聲明(성명)설명 金外務長官(김외무장관)\n",
      "日(일)매스콤 톱뉴스\n",
      "中央情報部(중앙정보부)는 超緊張(초긴장)\n",
      "與黨議員(여당의원)들 놀라기만\n",
      "金總理(김총리)는 閣僚(각요)들과…\n",
      "非常(비상)아래 이럴수가…\n",
      "余錄(여록)\n",
      "洪吉童(홍길동) (89)\n",
      "\"나는 왜 平壤(평양)에 갔었다…\" 李厚洛情報部長(이후락정보부장)이 밝힌 南北共同聲明(남북공동성명)의 背景(배경)\n",
      "李厚洛(이후낙) 中央情報部長(중앙정보부장)\n",
      "金英柱(김영주) 勞動黨組織部長(로동당조직부장)\n",
      "朴成哲(박성철) 第二副首相(제이부수상)\n",
      "南北(남북) 共同聲明(공동성명) 세 산파役(역)의 프로필\n",
      "印(인)·파不侵協定(부침협정)의 체결\n",
      "社説(사설)\n",
      "長針短針(장침단침)\n",
      "횡설수설\n",
      "越南軍(월남군) 쾅트리市入城(시입성)\n",
      "쾅트리 南(남)쪽서 탱크戰(전) 벌어져\n",
      "「EC頂上會談(정상회담)」논의\n",
      "美機雷(미기뇌)수백개 또 投下(투하) 越盟通信(월맹통신)보도\n",
      "東南亞(동남아)에美軍五萬投入(미군오만투입) 越盟攻勢(월맹공세)이래\n",
      "점보제트機(기) 拉致犯(랍치범) 在美(재미)월남인 留學生(유학생)\n",
      "駐越軍(주월군) 戰術(전술)지역 擴張(확장) 李司令官(리사령관)발표\n",
      "론놀大統領(대통영)취임식 首相職(수상직)겸직\n",
      "\"外侵(외침)에대한鬪爭(투쟁)계속\"\n",
      "\"北爆(북폭)확대 中共(중공)위협\n",
      "빠리平和會談(평화회담)결실위해 全美軍(전미군)무조건撤收(철수)해야빈 베트콩代表(대표)말\n",
      "몰몬教主(교주)스미드씨死亡(사망)\n",
      "美(미)、印支(인지)서 비밀리에 氣象戰(기상전)\n",
      "卋界(세계)의톱뉴스\n",
      "美(미)에 척추病(병)늘어나 매년 새환자一五(일오)○萬發生(만발생)\n",
      "和(화)、新聞(신문)에 補助金(보조김) 광고激減(격감)으로 財政難(재정난)\n",
      "?????外(외)토픽\n",
      "南北(남북)의對話(대화)<108>在日(재일) 同胞社會(동포사회)의 左右對立(좌우대립)⑥\n",
      "\"오까모도 釋放(석방)안하면報復(보복)\" 國際航空協會(국제항공협회)에 협박文(문)\n",
      "客舎(객사)(122)\n",
      "\"와야할일 오고야 말았다\" 「南北共同(남북공동)성명」의「충격發表(발표)」를 분석하는緊急座談會(긴급좌담회)\n",
      "一九四(일구사)개團體(단체) 강제解散市(해산시)\n",
      "라디오프로\n",
      "各地方(각지방)의 表情(표정)\n",
      "斷絕(단절)의 사슬 풀「南北(남북)의 青信號(청신호)」그들이 오간 統一路(통일로) 「7·4南北共同聲明(남북공동성명) 發表(발표)되던 날  民族念願(민족념원)품고疾走(질주)한統一(통일)에의 새길\n",
      "中部(중부)에 豪雨(호우)주의보\n",
      "斷絕(단절)의 사슬 풀「南北(남북)의 青信號(청신호)」그들이 오간 統一路(통일로) 「7·4南北共同聲明(남북공동성명) 發表(발표)되던 날  民族念願(민족념원)품고疾走(질주)한統一(통일)에의 새길\n",
      "統一前兆(통일전조)의 感激(감격) 五道廳(오도청) 會談(회담) 더욱잘될것 韓赤(한적)\n",
      "곳곳서 물난리 셋死亡(사망) 셋失踪(실종)\n",
      "냉철한 對備(대비)를 各界反應(각계반응) 새事態(사태) 對策(대책)시급\n",
      "韓銀(한은)마닐라支店(지점)에 불\n",
      "휴지통\n",
      "「黃金(황김)사자旗(기) 高校(고교)야구」 歷代(력대) 優勝(우승)팀 편람\n",
      "◇歷代(역대)우승校(교)와 決勝(결승)전적\n",
      "뮨헨올림픽의 다크호스「美國柔道(미국유도)」招請渡美(초청도미)한 朴俊容(박준용)유도회理事(리사)의 傳信(전신)\n",
      "스포츠話題(화제)\n",
      "「베토벤」·「바하」作品(작품) 出盤(출반) 성음社(사),선명會(회) 公演曲(공연곡)도\n",
      "慶尙道(경상도) 新人(신인)배우모집 「女高時節(여고시절)」映畵化(영화화)에\n",
      "演芸(연운)수첩\n",
      "南北(남북)접촉관계 報道特集(보도특집) ★‥‥DBS放送(방송)·TV3局(국)\n",
      "「제프의 護身術(호신술)」편 ★‥‥TBC「도나 리드 쇼」\n",
      "「90秒戰爭(묘전쟁)」前篇(전편) ★‥‥KBS「5-0수사대」\n",
      "라디오TV하일라이트\n",
      "8月(월)10日壯途(일장도)에 뮨헨올림픽韓國代表團(한국대표단)\n",
      "産銀(산은)·泰光(태광)각각勝利(승리)\n",
      "韓銀(한은)、빅토리에 설욕\n",
      "周(주)창석·李(이)에리사優勝(우승) 全國(전국)실업卓球單式(탁구단식)\n",
      "棒高跳(봉고도)┄5m63 美(미) 봅 시그린선수\n",
      "世界新記録(세계신기녹)\n",
      "제17기 国手战(국수전) 挑战者(도전자)결정本选(본선)리그\n",
      "소비자보호 캠페인 ➎\n",
      "소비자를보호하자\n"
     ]
    }
   ],
   "source": [
    "#기초 웹 크롤링 예시 - 아카이브 크롤링\n",
    "\n",
    "#로그인이 필요한 사이트의 크롤링은 셀레니움을 사용해야 했었다. 하지만 뷰티풀 수프를 사용해도 된다.\n",
    "#뷰티풀 수프를 사용하면 셀레니움에 비해 속도가 빠르지만 , 관련 지식이나 세부적인 내용에 대해 이해를 필요로 한다.\n",
    "\n",
    "#로그인이 필요한 사이트 , 이번 예시에서는 아카이브 사이트를 사용한다. 이 사이트에서 로그인창에서\n",
    "#id와 비밀번호를 입력한후 , 네트워크 에서 Preserve Log 를 체크한 후 로그인을 진행한다.\n",
    "#그럼 다양한 서버와 주고받은 데이터들이 히스토리로 나온다. 가장 위쪽이 가장 과거의 것이다.\n",
    "#예를 들어 로그인 버튼을 누르는 순간 로그인 해줘 라는 요청이 서버에 가게 된 것 이다.\n",
    "\n",
    "#로그인 쪽을 확인하면 get과 post로 나뉘게 된다.\n",
    "#get -> 서버에 요청시 요청 메세지를 함께 실어 보내고 간편하지만 보안에 취약하다.\n",
    "#post -> 요청 메세지를 보내지만 따로 보안을 해서 보낸다.\n",
    "#그래서 대부분의 데이터 교환에는 get을 사용하지만 보안이 중요한 아이디 , 비밀번호 , 로그인에 대해서는 post를 사용한다.\n",
    "\n",
    "#결론적으로 우리가 로그인에 대해서 알 수 있는 것은 아 이 url로 post 방식으로 로그인을 해야 하는 구나 를 알 수 있다.\n",
    "#맨 아래쪽의 form data를 확인하면 서버에 어떤 데이터를 보냈는지 확인 할 수 있다.\n",
    "#확인하면 url주소와 id , 비밀번호 가 들어있다. url 주소는 로그인 성공시 보게 되는 url 주소이다.\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "sess = requests.session()\n",
    "#로그인을 위해 서버와 컴퓨터 사이의 연결고리를 만든다. 이를 통해 로그인을 시도할 수 있다.\n",
    "\n",
    "data1 ={\n",
    "    \"idsave_value\": \"\", \n",
    "    \"errorChk\": \"\", \n",
    "    \"gourl\": \"https%3A%2F%2Fwww.donga.com%2Farchive%2Fnewslibrary%2Fview%3Fymd%3D19720704%26mode%3D19720704%252F0001266487%252F1\",\n",
    "    \"bid\": \"talingpython\",\n",
    "    \"bpw\": \"xkfdldvkdlTjs2\"\n",
    "}\n",
    "\n",
    "sess.post(\"https://secure.donga.com/membership/trans_exe.php\", data=data1)\n",
    "#세션.post는 post요청을 하는 함수이다.\n",
    "#위 함수의 첫번째 인자는 요청할 서버의 주소를 써주면 된다. 즉 제너럴의 리퀘스트 url을 입력하면 된다.\n",
    "#두번째 인자는 data= 이라는 데이터 매개변수에 data라는 딕셔너리 자료형을 넣는다.\n",
    "#data 안에는 form data를 넣어 보낼 것이다.\n",
    "\n",
    "#위처럼 post 요청을 보내면 서버가 로그인을 진행하게 된다.\n",
    "#이후 로그인이 완료되면 우리가 보고 싶은 정보들을 볼 수 있게 되는 것이다.\n",
    "\n",
    "#로그인이 완료되었으니 이제 urlopen하면 된다고 생각할 수 있지만 세션이 남아있다.\n",
    "#그냥 진행하면 세션 연결이 끊어질 수 있다.\n",
    "#따라서 세션.get() 이라는 함수를 써서 이 연결을 이어 나가야 한다.\n",
    "#이 get이라는 함수는 urlopen과 같은 기능을 수행한다. 다른 점은 이 로그인 한 것을 유지하기 위함 이다.\n",
    "#html 코드를 받아온다는 것은 다르지 않다.\n",
    "\n",
    "code = sess.get(\"https://www.donga.com/archive/newslibrary/view?ymd=19720704&mode=19720704%2F0001266487%2F1\")\n",
    "soup = BeautifulSoup(code.text, \"html.parser\")\n",
    "\n",
    "#urlopen과 세션.get()의 차이가 뷰티풀 수프를 사용하면서 하나 더 보이는데 바로 세션은 코드를 받아올 떄 \n",
    "#code.text를 사용해야 한다는 것이다. urlopen은 그냥 code로 진행했었다.\n",
    "\n",
    "title = soup.select(\"ul.news_list a\")\n",
    "for i in title:\n",
    "    print(i.string)\n",
    "\n",
    "#이처럼 로그인이 필요한 크롤링을 뷰티풀 수프를 통해서 진행해보았다.\n",
    "#http 호출에 자주 쓰이는 requests 모듈에 대한 이해가 중요하다.\n",
    "\n",
    "#만약 페이지 url이 바뀌지 않고 한 화면위에서 동적으로 바뀌는 경우,\n",
    "#그런데 html 코드에서 단서를 찾을 수 없는 경우 네트워크 부분을 다시 찾아본다.\n",
    "#클릭시 url 이동이 안된다면 네트워크 히스토리가 남을 것이다.\n",
    "#그 부분을 확인하면 해당 페이지의 url 주소를 알 수 있다.\n",
    "\n",
    "#만약 해당하는 url을 찾아서 for문을 돌려보려고 하는데 규칙성이 보이지 않는경우, 자바스크립트를 확인 해볼 수 있다.\n",
    "#위의 예시에서는 자바 스크립트의 코드 인자로 해당하는 url의 규칙을 가진 수들이 들어가있다.\n",
    "#해당하는 수들을 모두 받아와 네트워크 url과 비교해 해당하는 url들을 모두 찾을 수 있다.\n",
    "\n",
    "#위의 예시에서는 로그인시 헤더 정보를 더 넣어줘야 뉴스 본문을 크롤링 할 수 있다.\n",
    "#그래서 Request header 전체를 다 넣는 것도 방법이 될 수 있다.\n",
    "#헤더는 data와 다르게 다른 인자로 받아야한다.\n",
    "#sess.post(\"https://secure.donga.com/membership/trans_exe.php\", headers=헤더,data=data1)\n",
    "#위와 같이 헤더를 포함해서 데이터를 보낼 수 있다.\n",
    "\n",
    "#크롤링을 할때 robots.txt를 확인해야 하는 경우가 생길 수 있다.\n",
    "#url 뒤에 /robots.txt 를 입력해 robots.txt 파일을 확인 할 수 있다.\n",
    "#파일에는 어떤 유저에 대해서 허용하는지 , 어떤 페이지를 허용하고 허용하지 않는지에 대한 내용이 들어있다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
